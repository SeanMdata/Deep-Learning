{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "from keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data\n",
    "\n",
    "Data Source: https://www.kaggle.com/datasets/gpiosenka/100-bird-species\n",
    "\n",
    "Initial data had 525 species of birds with 100 to 200 images per species in the training set. To make the training times more reasonable, we reduced the data down to 50 randomly selected species and only the first 100 training images for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path for loading data\n",
    "train_dir = \"C:/Users/bcbot/Deep-Learning/birds/train\"\n",
    "val_dir = \"C:/Users/bcbot/Deep-Learning/birds/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the containers for holding image and label data\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Load the first 100 files for each bird for training set\n",
    "for i in os.listdir(train_dir):\n",
    "    sub_directory = os.path.join(train_dir, i)\n",
    "    for j in os.listdir(sub_directory):\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        train_data.append([img, i])\n",
    "\n",
    "# Load first 50 files for each bird in validation set\n",
    "for i in os.listdir(val_dir):\n",
    "    sub_directory = os.path.join(val_dir, i)\n",
    "    for j in os.listdir(sub_directory):\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        val_data.append([img, i])\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 images of different shape in the train data.\n",
      "There are 0 images of different shape in the validation data.\n"
     ]
    }
   ],
   "source": [
    "# Validating that all images are the same shape\n",
    "\n",
    "count_t = 0\n",
    "count_v = 0\n",
    "\n",
    "for t in train_data:\n",
    "    if t[0].shape != (224, 224, 3):\n",
    "        count_t += 1\n",
    "    \n",
    "for v in val_data:\n",
    "    if v[0].shape != (224, 224, 3):\n",
    "        count_v += 1\n",
    "\n",
    "print(f'There are {count_t} images of different shape in the train data.\\nThere are {count_v} images of different shape in the validation data.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "np.random.seed(7284)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data and validation data\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for x, y in train_data:\n",
    "    X_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train_vect = lb.fit_transform(y_train)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for x, y in val_data:\n",
    "    X_val.append(x)\n",
    "    y_val.append(y)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_val_vect = lb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Baseline Model\n",
    "\n",
    "The dataset authros noted that they had achieved good success using EfficientNetB0, so we decided to use that as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping parameters\n",
    "callback = EarlyStopping(monitor= 'val_accuracy', patience= 5, start_from_epoch= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Model \n",
    "enet = EfficientNetB0(include_top= False, input_shape=(224, 224, 3))\n",
    "\n",
    "layers = enet.layers\n",
    "\n",
    "for layer in layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a flatten and softmax layer\n",
    "model= Sequential([\n",
    "    enet,\n",
    "    Flatten(),\n",
    "    Dense(50, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "63/63 [==============================] - 41s 591ms/step - loss: 1.9436 - accuracy: 0.7448 - val_loss: 0.2981 - val_accuracy: 0.9360\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 35s 551ms/step - loss: 0.2495 - accuracy: 0.9592 - val_loss: 0.4441 - val_accuracy: 0.9280\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 36s 573ms/step - loss: 0.1228 - accuracy: 0.9782 - val_loss: 0.5983 - val_accuracy: 0.9240\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 35s 558ms/step - loss: 0.0687 - accuracy: 0.9878 - val_loss: 0.4944 - val_accuracy: 0.9480\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 35s 554ms/step - loss: 0.0984 - accuracy: 0.9846 - val_loss: 0.3774 - val_accuracy: 0.9360\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.0558 - accuracy: 0.9886 - val_loss: 0.5302 - val_accuracy: 0.9400\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 37s 588ms/step - loss: 0.0770 - accuracy: 0.9874 - val_loss: 0.4072 - val_accuracy: 0.9520\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 39s 619ms/step - loss: 0.0784 - accuracy: 0.9886 - val_loss: 0.6505 - val_accuracy: 0.9080\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 42s 672ms/step - loss: 0.0811 - accuracy: 0.9878 - val_loss: 0.5329 - val_accuracy: 0.9240\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 33s 523ms/step - loss: 0.1327 - accuracy: 0.9818 - val_loss: 0.7341 - val_accuracy: 0.9320\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 35s 550ms/step - loss: 0.1083 - accuracy: 0.9848 - val_loss: 0.9822 - val_accuracy: 0.9320\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 34s 548ms/step - loss: 0.1175 - accuracy: 0.9862 - val_loss: 1.3013 - val_accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y= y_train_vect, batch_size=80, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Problem Difficulty\n",
    "\n",
    "To ensure the smaller dataset didn't make the problem too easy, we ran it through a very simple model to see what a baseline starting point for our built-from-scratch model would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with 1 CNN layer, 1 pooling layer, and a softmax layer\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, input_shape = (224, 224, 3), activation= 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 17s 259ms/step - loss: 8.5220 - accuracy: 0.1320 - val_loss: 2.5930 - val_accuracy: 0.3840\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 1.5623 - accuracy: 0.6462 - val_loss: 1.6337 - val_accuracy: 0.5480\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 16s 261ms/step - loss: 0.3205 - accuracy: 0.9398 - val_loss: 1.5852 - val_accuracy: 0.5600\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.0561 - accuracy: 0.9956 - val_loss: 1.4997 - val_accuracy: 0.6120\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.0182 - accuracy: 0.9992 - val_loss: 1.4290 - val_accuracy: 0.6160\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 16s 255ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.4246 - val_accuracy: 0.6360\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 17s 263ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.4353 - val_accuracy: 0.6400\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 18s 285ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.4360 - val_accuracy: 0.6400\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.4533 - val_accuracy: 0.6320\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.4455 - val_accuracy: 0.6440\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.4570 - val_accuracy: 0.6400\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 17s 269ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4572 - val_accuracy: 0.6400\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.4411 - val_accuracy: 0.6400\n",
      "Epoch 14/30\n",
      "63/63 [==============================] - 17s 267ms/step - loss: 8.7995e-04 - accuracy: 1.0000 - val_loss: 1.4446 - val_accuracy: 0.6280\n",
      "Epoch 15/30\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 7.7641e-04 - accuracy: 1.0000 - val_loss: 1.4374 - val_accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(x = X_train/255, y= y_train_vect, batch_size=80, validation_data= (X_val/255, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
