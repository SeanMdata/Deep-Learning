{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Input, Flatten, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import opendatasets as od\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "from keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data\n",
    "\n",
    "Notes about data:\n",
    "\n",
    "1) the file PARAKETT AUKLET had a few typos. In the Train and Test sets it had two spaces inbetween words, but only one in the Valid set. It should also be PARAKEET. I manually updated this in my version of the files. \n",
    "\n",
    "2) The valid and test datasets only have 5 birds images for each bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od.download('https://www.kaggle.com/datasets/gpiosenka/100-bird-species/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path for loading data\n",
    "train_dir = 'C:/Users/bcbot/COMP_4531/Final/100-bird-species/train'\n",
    "val_dir = \"C:/Users/bcbot/COMP_4531/Final/100-bird-species/valid\"\n",
    "test_dir = \"C:/Users/bcbot/COMP_4531/Final/100-bird-species/test\"\n",
    "\n",
    "train_sav = \"C:/Users/bcbot/Deep-Learning/birds/train\"\n",
    "val_sav = \"C:/Users/bcbot/Deep-Learning/birds/valid\"\n",
    "test_sav = \"C:/Users/bcbot/Deep-Learning/birds/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code just randomly selects some number of birds (specificed by the num_birds varaible). \n",
    "# This can be adjusted to speed up training, but need to make sure the softmax layer of the model matches the num_birds\n",
    "\n",
    "num_birds = 50\n",
    "\n",
    "birds = [i for i in os.listdir(train_dir)]\n",
    "\n",
    "random.seed(7284)\n",
    "rand_idxes = random.sample(range(0, len(birds)), num_birds)\n",
    "\n",
    "include = [birds[i] for i in rand_idxes]\n",
    "\n",
    "len(include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the containers for holding image and label data\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "# Load the first 100 files for each bird for training set\n",
    "for i in include:\n",
    "    count = 0\n",
    "    sub_directory = os.path.join(train_dir, i)\n",
    "\n",
    "    # # Code to create local directory\n",
    "    # sav_directory = os.path.join(train_sav, i)\n",
    "    # os.mkdir(sav_directory)\n",
    "\n",
    "    for j in os.listdir(sub_directory):\n",
    "        count += 1\n",
    "        if count > 100:\n",
    "            break\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        train_data.append([img, i])\n",
    "\n",
    "        # # Code to save images locally\n",
    "        # filename = os.path.join(sav_directory, j)\n",
    "        # cv2.imwrite(filename, img)\n",
    "\n",
    "\n",
    "\n",
    "# Load images for validation set\n",
    "for i in include:\n",
    "    sub_directory = os.path.join(val_dir, i)\n",
    "\n",
    "    # # Code to create local directory\n",
    "    # sav_directory = os.path.join(val_sav, i)\n",
    "    # os.mkdir(sav_directory)\n",
    "\n",
    "    for j in os.listdir(sub_directory):\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        val_data.append([img, i])\n",
    "\n",
    "        # # Code to save images locally\n",
    "        # filename = os.path.join(sav_directory, j)\n",
    "        # cv2.imwrite(filename, img)\n",
    "\n",
    "#Load images for test set\n",
    "for i in include:\n",
    "    sub_directory = os.path.join(test_dir, i)\n",
    "\n",
    "    # # Code to create local directory\n",
    "    # sav_directory = os.path.join(test_sav, i)\n",
    "    # os.mkdir(sav_directory)\n",
    "\n",
    "    for j in os.listdir(sub_directory):\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        test_data.append([img, i])\n",
    "\n",
    "        # # Code to save images locally\n",
    "        # filename = os.path.join(sav_directory, j)\n",
    "        # cv2.imwrite(filename, img)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all images are the same shape\n",
    "# NOTE: In this particular set, all the iamges are (224, 224, 3), but when I loaded files from all 525 birds there were some that were not, \n",
    "# and those screw up the model. So we may need to reshape if we use more birds\n",
    "\n",
    "count_t = 0\n",
    "count_v = 0\n",
    "\n",
    "for t in train_data:\n",
    "    if t[0].shape != (224, 224, 3):\n",
    "        count_t += 1\n",
    "    \n",
    "for v in val_data:\n",
    "    if v[0].shape != (224, 224, 3):\n",
    "        count_v += 1\n",
    "\n",
    "print(f'There are {count_t} images of different shape in the train data.\\nThere are {count_v} images of different shape in the validation data.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images of the wrong size if needed\n",
    "\n",
    "if count_t != 0:\n",
    "    for x in train_data:\n",
    "        if x[0].shape != (224, 224, 3):\n",
    "            x[0] = cv2.resize(x[0], (224, 224))\n",
    "\n",
    "if count_v != 0:\n",
    "    for x in val_data:\n",
    "        if x[0].shape != (224, 224, 3):\n",
    "            x[0] = cv2.resize(x[0], (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "np.random.seed(7284)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data and validation data\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for x, y in train_data:\n",
    "    X_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train_vect = lb.fit_transform(y_train)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for x, y in val_data:\n",
    "    X_val.append(x)\n",
    "    y_val.append(y)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_val_vect = lb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "\n",
    "I initally tried the pre-trained model, but I don't know how to change the number of outputs in the last layer, and trying to run with all 525 species was taking forever. I put some notes on that run but I think I'm just going to give up on trying to use that one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping parameters\n",
    "callback = EarlyStopping(monitor= 'val_accuracy', patience= 5, start_from_epoch= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for attempt with all 525 birds on pre-trained model:\n",
    "\n",
    "I tried running this and it earlystopped at 11 epochs. The val_Accuracy never got better than .12 (on epoch 6) but pretty much stayed in the < .01 range. Still not sure how you're supposed to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained model provided by dataset authors\n",
    "\n",
    "# model_path = 'C:/Users/bcbot/COMP_4531/Final/100-bird-species/EfficientNetB0-525-(224 X 224)- 98.97.h5'\n",
    "# model= keras.models.load_model(model_path, custom_objects={'F1_score':'F1_score'})\n",
    "\n",
    "# layers = model.layers\n",
    "# for layer in layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing the output layer = model.layers[-2].output\n",
    "# new_output = model.layers[-2].output\n",
    "# new_layer = Dense(num_birds, activation='relu')(new_output)\n",
    "\n",
    "# # creating a new output layer w/ 12 nodes for the subset of data I chose\n",
    "# model = Model(inputs=model.input, outputs= new_layer)\n",
    "\n",
    "\n",
    "# model.compile(optimizer ='adam',\n",
    "#                loss = 'categorical_crossentropy',\n",
    "#                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance of model on training data to evaluate provided weights\n",
    "\n",
    "# history = model.fit(x = X_train, y= y_train_vect, batch_size=32, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "A note on this. The EfficientNet model includes a image processing step that expects the pixels to be in the 0-255 range, I had originally been normalizing my pixels by dividing by 255 and that was resulting in terrible results (like 2% accuracy), as soon as I removed the normalization the accuracy jumped up to 80%. \n",
    "\n",
    "First test - 100 Birds from 50 species\n",
    "\n",
    "Each epoch took about 2:30 minutes with a batch size of 80. I let it run for about 25 minute (11 epochs) and I was hovering around 85% accuracy. I ran it again after I froze the first 100 layers, it cut the time per epoch in half and accuracy didn't seem to suffer, I let that one run for 6 epochs and got accuracy at 85+ for all of them. I tried running it again with all of the layers frozen (except the ones I added) and that cut training time per epoch in half again (only 30s per epoch) and I got better accuracy results than with any of the runs. I let that one run until it earlystopped at epoch 11 and none of my accuracy results were less than 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an Elasticnet model for baseline\n",
    "\n",
    "# Load Model \n",
    "enet = EfficientNetB0(include_top= False, input_shape=(224, 224, 3))\n",
    "\n",
    "layers = enet.layers\n",
    "\n",
    "for layer in layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a flatten and softmax layer\n",
    "model= Sequential([\n",
    "    enet,\n",
    "    Flatten(),\n",
    "    Dense(num_birds, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = X_train, y= y_train_vect, batch_size=80, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for attempt with all 525 birds:\n",
    "\n",
    "I manually stopped this one after it was running for about 2.5 hours with a batch size of 32. This one never got great accuracy, but was pretty consistently performing between .12 and .14 over 18 epochs. So it's already performing better than the provided model, which is just confusing me more. \n",
    "\n",
    "Notes for 100 birds from 50 species:\n",
    "\n",
    "This took about 20 seconds per epoch with a batch size of 80. Early stopping kicked in at epcoh 13. The model pretty much just hung out at a val accuracy of .60ish for all the epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a very simple model to see how it performs\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, input_shape = (224, 224, 3), activation= 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(.2),\n",
    "    Flatten(),\n",
    "    Dense(num_birds, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(x = X_train/255, y= y_train_vect, batch_size=80, validation_data= (X_val/255, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
