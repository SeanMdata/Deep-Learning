{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Input, Flatten, Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import opendatasets as od\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "from keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Data\n",
    "\n",
    "Notes about data:\n",
    "\n",
    "1) the file PARAKETT AUKLET had a few typos. In the Train and Test sets it had two spaces inbetween words, but only one in the Valid set. It should also be PARAKEET. I manually updated this in my version of the files. \n",
    "\n",
    "2) The valid and test datasets only have 5 birds images for each bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od.download('https://www.kaggle.com/datasets/gpiosenka/100-bird-species/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path for loading data\n",
    "train_dir = 'C:/Users/bcbot/COMP_4531/Final/100-bird-species/train'\n",
    "val_dir = \"C:/Users/bcbot/COMP_4531/Final/100-bird-species/valid\"\n",
    "test_dir = \"C:/Users/bcbot/COMP_4531/Final/100-bird-species/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code just randomly selects some number of birds (specificed by the num_birds varaible). \n",
    "# This can be adjusted to speed up training, but need to make sure the softmax layer of the model matches the num_birds\n",
    "\n",
    "num_birds = 50\n",
    "\n",
    "birds = [i for i in os.listdir(train_dir)]\n",
    "\n",
    "random.seed(7284)\n",
    "rand_idxes = random.sample(range(0, len(birds)), num_birds)\n",
    "\n",
    "include = [birds[i] for i in rand_idxes]\n",
    "\n",
    "len(include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMU', 57]\n",
      "['NORTHERN BEARDLESS TYRANNULET', 83]\n"
     ]
    }
   ],
   "source": [
    "minB = ['', np.inf]\n",
    "maxB = ['', 0]\n",
    "for i in os.listdir(val_dir):\n",
    "    count = 0\n",
    "    sub_directory = os.path.join(val_dir, i)\n",
    "    for j in sub_directory:\n",
    "        count += 1\n",
    "    if count < minB[1]:\n",
    "        minB = [i, count]\n",
    "    if count > maxB[1]:\n",
    "        maxB = [i, count]\n",
    "\n",
    "print(minB)\n",
    "print(maxB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the containers for holding image and label data\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Load the first 100 files for each bird for training set\n",
    "for i in include:\n",
    "    count = 0\n",
    "    sub_directory = os.path.join(train_dir, i)\n",
    "    for j in os.listdir(sub_directory):\n",
    "        count += 1\n",
    "        if count > 100:\n",
    "            break\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        train_data.append([img, i])\n",
    "\n",
    "# Load first 50 files for each bird in validation set\n",
    "for i in include:\n",
    "    sub_directory = os.path.join(val_dir, i)\n",
    "    for j in os.listdir(sub_directory):\n",
    "        img = cv2.imread(os.path.join(sub_directory, j))\n",
    "        val_data.append([img, i])\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 images of different shape in the train data.\n",
      "There are 0 images of different shape in the validation data.\n"
     ]
    }
   ],
   "source": [
    "# Check if all images are the same shape\n",
    "# NOTE: In this particular set, all the iamges are (224, 224, 3), but when I loaded files from all 525 birds there were some that were not, \n",
    "# and those screw up the model. So we may need to reshape if we use more birds\n",
    "\n",
    "count_t = 0\n",
    "count_v = 0\n",
    "\n",
    "for t in train_data:\n",
    "    if t[0].shape != (224, 224, 3):\n",
    "        count_t += 1\n",
    "    \n",
    "for v in val_data:\n",
    "    if v[0].shape != (224, 224, 3):\n",
    "        count_v += 1\n",
    "\n",
    "print(f'There are {count_t} images of different shape in the train data.\\nThere are {count_v} images of different shape in the validation data.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape images of the wrong size\n",
    "\n",
    "# for x in train_data:\n",
    "#     if x[0].shape != (224, 224, 3):\n",
    "#         x[0] = cv2.resize(x[0], (224, 224))\n",
    "\n",
    "# for x in val_data:\n",
    "#     if x[0].shape != (224, 224, 3):\n",
    "#         x[0] = cv2.resize(x[0], (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "np.random.seed(7284)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data and validation data\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for x, y in train_data:\n",
    "    X_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train_vect = lb.fit_transform(y_train)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for x, y in val_data:\n",
    "    X_val.append(x)\n",
    "    y_val.append(y)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_val_vect = lb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "\n",
    "I initally tried the pre-trained model, but I don't know how to change the number of outputs in the last layer, and trying to run with all 525 species was taking forever. I put some notes on that run but I think I'm just going to give up on trying to use that one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for attempt with all 525 birds on pre-trained model:\n",
    "\n",
    "I tried running this and it earlystopped at 11 epochs. The val_Accuracy never got better than .12 (on epoch 6) but pretty much stayed in the < .01 range. Still not sure how you're supposed to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained model provided by dataset authors\n",
    "\n",
    "# model_path = 'C:/Users/bcbot/COMP_4531/Final/100-bird-species/EfficientNetB0-525-(224 X 224)- 98.97.h5'\n",
    "# bmod= keras.models.load_model(model_path, custom_objects={'F1_score':'F1_score'})\n",
    "\n",
    "# # Check the weights\n",
    "\n",
    "# bmod.weights\n",
    "# # Check performance of model on training data to evaluate provided weights\n",
    "\n",
    "# bmod.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "# bmod.evaluate(X_train, y_train_vect)\n",
    "\n",
    "# history = bmod.fit(x = X_train, y= y_train_vect, batch_size=32, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping parameters\n",
    "callback = EarlyStopping(monitor= 'val_accuracy', patience= 5, start_from_epoch= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "A note on this. The EfficientNet model includes a image processing step that expects the pixels to be in the 0-255 range, I had originally been normalizing my pixels by dividing by 255 and that was resulting in terrible results (like 2% accuracy), as soon as I removed the normalization the accuracy jumped up to 80%. \n",
    "\n",
    "First test - 100 Birds from 50 species\n",
    "\n",
    "Each epoch took about 2:30 minutes with a batch size of 80. I let it run for about 25 minute (11 epochs) and I was hovering around 85% accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an Elasticnet model for baseline\n",
    "\n",
    "# Load Model \n",
    "enet = EfficientNetB0(include_top= False, input_shape=(224, 224, 3))\n",
    "\n",
    "layers = enet.layers\n",
    "\n",
    "for layer in layers[:20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a flatten and softmax layer\n",
    "model= Sequential([\n",
    "    enet,\n",
    "    Flatten(),\n",
    "    Dense(num_birds, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 130s 2s/step - loss: 1.2576 - accuracy: 0.7512 - val_loss: 1.3375 - val_accuracy: 0.8040\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 111s 2s/step - loss: 0.1304 - accuracy: 0.9692 - val_loss: 0.6114 - val_accuracy: 0.8840\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 126s 2s/step - loss: 0.1273 - accuracy: 0.9764 - val_loss: 0.6031 - val_accuracy: 0.9160\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 144s 2s/step - loss: 0.0829 - accuracy: 0.9828 - val_loss: 1.0584 - val_accuracy: 0.8920\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 142s 2s/step - loss: 0.1553 - accuracy: 0.9664 - val_loss: 1.1600 - val_accuracy: 0.8640\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 140s 2s/step - loss: 0.2092 - accuracy: 0.9598 - val_loss: 2.5551 - val_accuracy: 0.8280\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 136s 2s/step - loss: 0.2360 - accuracy: 0.9576 - val_loss: 2.2351 - val_accuracy: 0.8440\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 179s 3s/step - loss: 0.3288 - accuracy: 0.9558 - val_loss: 2.3507 - val_accuracy: 0.8160\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 140s 2s/step - loss: 0.2727 - accuracy: 0.9610 - val_loss: 2.0182 - val_accuracy: 0.8560\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 122s 2s/step - loss: 0.2163 - accuracy: 0.9702 - val_loss: 4.2139 - val_accuracy: 0.8560\n",
      "Epoch 11/30\n",
      "51/63 [=======================>......] - ETA: 24s - loss: 0.1548 - accuracy: 0.9765"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(x \u001b[38;5;241m=\u001b[39m X_train, y\u001b[38;5;241m=\u001b[39m y_train_vect, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m (X_val, y_val_vect), verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, callbacks \u001b[38;5;241m=\u001b[39m [callback])\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y= y_train_vect, batch_size=80, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for attempt with all 525 birds:\n",
    "\n",
    "I manually stopped this one after it was running for about 2.5 hours with a batch size of 32. This one never got great accuracy, but was pretty consistently performing between .12 and .14 over 18 epochs. So it's already performing better than the provided model, which is just confusing me more. \n",
    "\n",
    "Notes for 100 birds from 50 species:\n",
    "\n",
    "This took about 20 seconds per epoch with a batch size of 80. Early stopping kicked in at epcoh 13. The model pretty much just hung out at a val accuracy of .60ish for all the epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try a very simple model to see how it performs\n",
    "model = Sequential([\n",
    "    Conv2D(32, 3, input_shape = (224, 224, 3), activation= 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(.2),\n",
    "    Flatten(),\n",
    "    Dense(num_birds, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bcbot\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 22s 344ms/step - loss: 1.5298e-04 - accuracy: 1.0000 - val_loss: 1.6577 - val_accuracy: 0.6160\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 21s 339ms/step - loss: 1.2877e-04 - accuracy: 1.0000 - val_loss: 1.6686 - val_accuracy: 0.6040\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 21s 331ms/step - loss: 1.1262e-04 - accuracy: 1.0000 - val_loss: 1.6766 - val_accuracy: 0.6040\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 20s 324ms/step - loss: 9.8916e-05 - accuracy: 1.0000 - val_loss: 1.6756 - val_accuracy: 0.6120\n",
      "Epoch 5/30\n",
      "63/63 [==============================] - 23s 358ms/step - loss: 8.6005e-05 - accuracy: 1.0000 - val_loss: 1.6848 - val_accuracy: 0.6080\n",
      "Epoch 6/30\n",
      "63/63 [==============================] - 22s 351ms/step - loss: 7.6679e-05 - accuracy: 1.0000 - val_loss: 1.6837 - val_accuracy: 0.6080\n",
      "Epoch 7/30\n",
      "63/63 [==============================] - 22s 352ms/step - loss: 6.9401e-05 - accuracy: 1.0000 - val_loss: 1.6934 - val_accuracy: 0.6080\n",
      "Epoch 8/30\n",
      "63/63 [==============================] - 23s 365ms/step - loss: 6.2021e-05 - accuracy: 1.0000 - val_loss: 1.7043 - val_accuracy: 0.6200\n",
      "Epoch 9/30\n",
      "63/63 [==============================] - 25s 389ms/step - loss: 5.5504e-05 - accuracy: 1.0000 - val_loss: 1.7021 - val_accuracy: 0.6160\n",
      "Epoch 10/30\n",
      "63/63 [==============================] - 21s 337ms/step - loss: 5.0588e-05 - accuracy: 1.0000 - val_loss: 1.7044 - val_accuracy: 0.6080\n",
      "Epoch 11/30\n",
      "63/63 [==============================] - 21s 334ms/step - loss: 4.6040e-05 - accuracy: 1.0000 - val_loss: 1.7165 - val_accuracy: 0.6160\n",
      "Epoch 12/30\n",
      "63/63 [==============================] - 21s 332ms/step - loss: 4.2055e-05 - accuracy: 1.0000 - val_loss: 1.7260 - val_accuracy: 0.6120\n",
      "Epoch 13/30\n",
      "63/63 [==============================] - 20s 322ms/step - loss: 3.8902e-05 - accuracy: 1.0000 - val_loss: 1.7275 - val_accuracy: 0.6160\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(x = X_train, y= y_train_vect, batch_size=80, validation_data= (X_val, y_val_vect), verbose = 1, epochs = 30, callbacks = [callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
